{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homeworks 5\n",
    "1. Implement feed-forward\n",
    "2. Hand-derived backpropagation for small neural network\n",
    "3. General-case backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# standard library\n",
    "import itertools\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "# this styling is purely my preference\n",
    "# less chartjunk\n",
    "sns.set_context('notebook', font_scale=1.5, rc={'line.linewidth': 2.5})\n",
    "sns.set(style='ticks', palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(X, betas, activ_func, out_activ_func=None):\n",
    "    \"\"\"Feeds the inputs to the network, given the weights \n",
    "    and the activation function. If the output activation\n",
    "    is not specified, assumes it's the same as the other\n",
    "    layers. Returns each layer's preactivation and activation.\"\"\"\n",
    "    if out_activ_func is None:\n",
    "        out_activ_func = activ_func  # activation for last \n",
    "    if len(X.shape) < 2:\n",
    "        X = X[np.newaxis, :]  # make it two dim\n",
    "    \n",
    "    # intialize preactivations \n",
    "    preactivs, activs = [], [X]\n",
    "    for layer, beta in enumerate(betas):\n",
    "        # iterate over betas, get the last activation\n",
    "        # and use it to compute the preactivations\n",
    "        last_activ = activs[-1] \n",
    "        \n",
    "        # also, pad wth bias\n",
    "        last_activ = np.hstack((np.ones((last_activ.shape[0], 1)), \n",
    "                                last_activ))\n",
    "        preactivs.append(last_activ.dot(beta.T))\n",
    "        \n",
    "        # pass it through the appropriate activ func\n",
    "        layer_func = activ_func if layer < len(betas) else out_activ_func\n",
    "        activs.append(layer_func(preactivs[-1]))  \n",
    "        \n",
    "    # return the preactivations and activations\n",
    "    return preactivs, activs\n",
    "    \n",
    "    \n",
    "def sigm(x):\n",
    "    \"\"\"Returns the sigmoid of the array/matrix\"\"\"\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[-2.3,  0.1,  3. ],\n",
       "         [-2. ,  3.5,  9.5]]), array([[ 0.93328831],\n",
       "         [ 1.20910962]])], [array([[1, 1, 0, 1],\n",
       "         [0, 1, 1, 3]]), array([[ 0.09112296,  0.52497919,  0.95257413],\n",
       "         [ 0.11920292,  0.97068777,  0.99992515]]), array([[ 0.71774194],\n",
       "         [ 0.77014137]])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out a 3, 2, 1 network\n",
    "X = np.array([[1, 1, 0, 1],\n",
    "              [0, 1, 1, 3]])\n",
    "betas = [np.array([[0.2, -1, -1, 0.3, -0.5],\n",
    "                   [0.1, -1, 0, 0.4, 1],\n",
    "                   [0, 0, 0, .5, 3]]),\n",
    "         np.array([[-0.3, 0.2, 0.5, 1]])]\n",
    "\n",
    "feed_forward(X, betas, sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(expected, preactivs, activs, betas, activ_grad_func, out_grad_func=None):\n",
    "    \"\"\"Computes and returns the gradient of the MLP, given the computed vals\n",
    "    the activations, preactivations, etas and gradient funcs for the hidden\n",
    "    and output layers.\n",
    "    They must all be received in matrix form!\n",
    "    \"\"\"\n",
    "    if out_grad_func is None:\n",
    "        # if unspecified, use the other activations \n",
    "        out_grad_func = activ_grad_func\n",
    "\n",
    "    # the error signal for the output nodes\n",
    "    # same for squared error and cross-entropy\n",
    "    pred_diff = activs[-1] - expected\n",
    "    err_signs = [pred_diff ]\n",
    "    # compute error signals for the rest, not for the input tho\n",
    "    for preactiv, beta in zip(preactivs[-2::-1], betas[:0:-1]):\n",
    "        preactiv_grad = activ_grad_func(preactiv)  # the gradient of the input\n",
    "        err_signs.append(preactiv_grad * err_signs[-1].dot(beta[:,1:]))  # no bias\n",
    "\n",
    "    err_signs = err_signs[::-1]  # reverse it\n",
    "    \n",
    "    grads = []\n",
    "    # compute the gradients from the error_signs \n",
    "    for err_sign, activ in zip(err_signs, activs[:-1]):\n",
    "        act = np.hstack((np.ones((activ.shape[0], 1)), activ))  # bias\n",
    "        grads.append(err_sign.T.dot(act) / expected.shape[0])\n",
    "        \n",
    "    return grads\n",
    "\n",
    "\n",
    "def sigm_grad(x):\n",
    "    \"\"\"Returns the sigmoid of the gradient function in x\"\"\"\n",
    "    return sigm(x) * (1 - sigm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.192 ,  0.0768]]), array([[ 0.32 ,  0.144]])],\n",
       " [array([[ 0.4]]), array([[ 0.45]]), array([[ 1.02]])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.4]])\n",
    "betas =  [np.array([[0.25, 0.5]]), np.array([[0.75, 0.60]])]\n",
    "preactivs, activs = feed_forward(X, betas, lambda x: x)\n",
    "\n",
    "# the result should be \n",
    "back_prop(np.array([[0.7]]), preactivs, activs, betas, lambda x: np.ones(x.shape)), activs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \"\"\"Class for a multilayer neural net\"\"\"\n",
    "    def __init__(self, layer_sizes, activ_func, activ_func_grad, out_activ_func=None, out_activ_func_grad=None):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activ_func = activ_func\n",
    "        self.out_activ_func = out_activ_func\n",
    "        self.activ_func_grad = activ_func_grad\n",
    "        self.out_activ_func_grad = out_activ_func_grad\n",
    "        \n",
    "    def init_weights(self, X, Y, minibatch_size=1):\n",
    "        # initialize the weights\n",
    "        self.betas = []\n",
    "        nodes_per_layer = [X.shape[1]] + self.layer_sizes + [Y.shape[1]]\n",
    "        for in_layer, out_layer in zip(nodes_per_layer[:-1], nodes_per_layer[1:]):\n",
    "            self.betas.append(np.random.normal(size=(out_layer, in_layer+1), scale=.001))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns the predicted values for the given input\"\"\"\n",
    "        preactivs, activs = feed_forward(X, self.betas, self.activ_func, self.out_activ_func)\n",
    "        return activs[-1]\n",
    "    \n",
    "    def get_grads(self, X, Y):\n",
    "        preactivs, activs = feed_forward(X, self.betas, self.activ_func, self.out_activ_func)\n",
    "        return back_prop(Y, preactivs, activs, self.betas, self.activ_func_grad, self.out_activ_func_grad)\n",
    "    \n",
    "    def train(self, X, Y, epochs=100, alpha=0.001):\n",
    "        self.init_weights(X, Y)\n",
    "        for i in range(epochs):\n",
    "            pos = i % X.shape[0]\n",
    "            X_batch, Y_batch = X[pos: pos+1], Y[pos: pos+1]\n",
    "            grads = self.get_grads(X_batch, Y_batch)\n",
    "            self.betas = [beta - alpha * grad for beta, grad in zip(self.betas, grads)]  # gradient descent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ -1.16512147e-03,  -6.34220542e-04,   9.02079215e-05]]),\n",
       "  array([[-0.0055869 , -0.00045346]]),\n",
       "  array([[ 0.80149321, -0.0053202 ]])],\n",
       " array([[ 0.80152293],\n",
       "        [ 0.80152293]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = np.array([[1, 0], [0, 1], [0, 0], [1, 1], [1, 0], [0, 1], [0, 0], [1, 1]])\n",
    "# Y = np.array([[1], [1], [0], [0], [1], [1], [0], [0]])\n",
    "# X, Y = sklearn.datasets.load_diabetes(return_X_y=True)\n",
    "# Y = Y[:, np.newaxis]\n",
    "# X = np.array([[1, 0], [0, 1], [0, 0], [1, 1], [1, 0], [0, 1], [0, 0], [1, 1]])\n",
    "# Y = np.array([[3], [4], [1], [6], [3], [4], [1], [6]])\n",
    "\n",
    "X = np.array([[0.4, 0.2], [0.5, 0.3]])\n",
    "Y = np.array([[0.7], [0.9]])\n",
    "\n",
    "mlp = MLP([1,1], lambda x: x, lambda x: np.ones(x.shape))\n",
    "mlp.train(X, Y, epochs=1000, alpha=0.03)\n",
    "mlp.betas, mlp.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
